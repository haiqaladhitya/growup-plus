# -*- coding: utf-8 -*-
"""Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14k6juMeuwi9OziVnD7pqmmhyom_b5v2J
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

import gdown
import pandas as pd

# Link unduhan langsung
file_id = '1k1c8CPBzX92phq5QKB225VlE7I-3OOuO'
url = f'https://drive.google.com/uc?id={file_id}'

gdown.download(url, 'stunting_wasting_dataset.csv', quiet=False)

df = pd.read_csv('stunting_wasting_dataset.csv', encoding='ISO-8859-1')

df.head()

df.isnull().sum()

df.info()

#Merubah kolom 'Jenis Kelamin' menjadi 1 atau 0 menggunakan label encoder

le = LabelEncoder()
df['Jenis Kelamin'] = le.fit_transform(df['Jenis Kelamin'])
df

"""Keterangan gender


*   0 = Laki - laki
*   1 = Perempuan



"""

df.info()

df.describe()

df_copy = df.copy()

stunting_map = {
    "Severely Stunted": 2,
    "Stunted": 1,
    "Normal": 0,
    "Tall": -1  #bakal dipake ga?
}
df_copy["Stunting"] = df_copy["Stunting"].map(stunting_map)

wasting_map = {
    "Severely Underweight": 2,
    "Underweight": 1,
    "Risk of Overweight": -1,
    "Normal weight": 0
}
df_copy["Wasting"] = df_copy["Wasting"].map(wasting_map)

df_copy

df_copy.isnull().sum()

df.info()

"""# **EDA**"""

numerical_features = df.select_dtypes(include=['int64', 'float64']).columns
plt.figure(figsize=(12, 8))
df_copy[numerical_features].hist(bins=30, figsize=(12, 8), color='c', edgecolor='k')
plt.suptitle("Histogram Distribusi Variabel Numerik")
plt.show()

"""Distribusi usia cukup merata pada setiap rentang umur dan jumlah data relatif seimbang untuk setiap kelompok umur

Distribusi tinggi badan berbentuk normal yang menunjukkan bahwa sebagian besar anak pada dataset memiliki rentang di 70-80 cm

Ditsribusi berat badan menyerupai distribusi normal dengan mayoritas aanak berada pada rentang 8-10 kg


Puncak distribusi pada kolom stunting menunjukkan bahwa sebagian besar anak adalah normal, sedangkan puncak distribusi pada kolom wasting menunjukkan bahwa sebagian besar anak berisiko kelebihan berat badan (overweight)

"""

plt.figure(figsize=(8, 5))
sns.boxplot(x=df_copy["Wasting"], y=df_copy["Umur (bulan)"], palette='coolwarm')
plt.title("Distribusi wasting Berdasarkan Usia")
plt.show()

"""Semua kategori memiliki median yang hampir sama yaitu sekitar 12 bulan, kategori Risk of Overweight dan Normal Weight memiliki distribusi usia yang lebih seragam dibandingkan kategori lainnya. sedangkan kategori Underweight dan Severely Underweight menunjukkan variasi yang lebih besar

Usia tidak terlalu mempengaruhi status wasting secara signifikan, karena median dan distribusi antar kategori hampir mirip
"""

plt.figure(figsize=(10, 6))
corr_matrix = df_copy.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Heatmap Korelasi Antar Variabel")
plt.show()

"""Faktor utama yang mempengaruhi wasting adalah berat badan (semakin rendah berat badan maka semakin tinggi kemungkinan wasting). umur sangat berpegaruh terhadap tinggi dan berat badan (semakin bertambah usia maka tinggi dan erat badan anak semaki bertambah). tinggi adan cenderung berpengaruh terhadap stunting (semakin pendek anak, maka cenderung mengalami stunting)"""

gender_analysis = df_copy.groupby("Jenis Kelamin")[['Tinggi Badan (cm)', 'Berat Badan (kg)']].mean()
print("Rata-rata tinggi & berat berdasarkan gender:\n", gender_analysis)

"""laki-laki memiliki rata-rata tinggi dan berat badan lebih tinggi daripada perempuan"""

plt.figure(figsize=(8, 5))
sns.boxplot(x=df_copy["Jenis Kelamin"], y=df_copy["Tinggi Badan (cm)"], palette='pastel')
plt.title("Distribusi Tinggi Berdasarkan Gender")
plt.show()

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.countplot(x='Stunting', hue='Jenis Kelamin', data=df_copy, palette='coolwarm')
plt.title("Distribusi Stunting Berdasarkan Gender")
plt.xlabel("Stunting Status")
plt.ylabel("Jumlah")

plt.subplot(1, 2, 2)
sns.countplot(x='Wasting', hue='Jenis Kelamin', data=df_copy, palette='coolwarm')
plt.title("Distribusi Wasting Berdasarkan Gender")
plt.xlabel("Wasting Status")
plt.ylabel("Jumlah")

plt.tight_layout()
plt.show()

"""pada status stunting, anak perempuan lebih banyak masuk dalam kategori stunted dan tall, sedangkan anak laki-laki lebih banyak yang severely stunting dan normal

pada wasting, anak perempuan sedikit lebih banyak dalam kategori risk of overweight sedangkan anak laki-laki lebih banyak pada kategori underweight dan severely underweight

namun, pada grafik ini perbedaannya tidak terlalu signifikan
"""

from sklearn.ensemble import RandomForestClassifier
#feature importance
X = df_copy[['Jenis Kelamin', 'Umur (bulan)', 'Tinggi Badan (cm)', 'Berat Badan (kg)']]
y = df_copy['Wasting']

# Membuat model Random Forest
model = RandomForestClassifier(random_state=42)
model.fit(X, y)

# Mengambil feature importance
feature_importances = model.feature_importances_

# Menampilkan feature importance
features = X.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print("Feature Importance:")
print(importance_df)

# Visualisasi feature importance
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
plt.gca().invert_yaxis()  # Membalik urutan agar yang paling penting di atas
plt.show()

"""#Modelling"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
import joblib
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

df_copy2 = df.copy()
df_copy2.head()

stunting_map = {
    "Severely Stunted": 2,
    "Stunted": 1,
    "Normal": 0,
    "Tall": 3  #bakal dipake ga?
}
df_copy2["Stunting"] = df_copy2["Stunting"].map(stunting_map)

wasting_map = {
    "Severely Underweight": 2,
    "Underweight": 1,
    "Risk of Overweight": 3,
    "Normal weight": 0
}
df_copy2["Wasting"] = df_copy2["Wasting"].map(wasting_map)

df_copy2.head()

"""##tensorflow"""

train, test = train_test_split(df_copy2, test_size=0.1, random_state=42)
train, val = train_test_split(train, test_size=0.15, random_state=42)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

def calculate_class_weights(label_column):
    unique_classes, class_counts = np.unique(label_column, return_counts=True)
    total_samples = len(label_column)
    class_weights = {}

    for class_label, class_count in zip(unique_classes, class_counts):
        class_weight = total_samples / (2.0 * class_count)
        class_weights[class_label] = class_weight

    return class_weights

class_weights = calculate_class_weights(train['Stunting'])
print("Class Weights:", class_weights)

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
    dataframe = dataframe.copy()
    labels = dataframe.pop('Stunting')
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size)
    return ds

feature_columns = []
for column_name in ['Jenis_Kelamin', 'Umur (bulan)', 'Tinggi Badan (cm)', 'Berat Badan (kg)']:
    feature_columns.append(tf.feature_column.numeric_column(column_name)) # Use tf.feature_column.numeric_column

batch_size = 256
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

# Calculate normalization statistics
umur_mean = train["Umur (bulan)"].mean()
umur_std = train["Umur (bulan)"].std()
tinggi_badan_mean = train["Tinggi Badan (cm)"].mean()
tinggi_badan_std = train["Tinggi Badan (cm)"].std()
berat_badan_mean = train["Berat Badan (kg)"].mean()
berat_badan_std = train["Berat Badan (kg)"].std()

# Print normalization statistics
print("Umur Mean:", umur_mean, "Umur Std:", umur_std)
print("Tinggi Badan Mean:", tinggi_badan_mean, "Tinggi Badan Std:", tinggi_badan_std)
print("Berat Badan Mean:", berat_badan_mean, "Berat Badan Std:", berat_badan_std)

# Initialize inputs
umur_input = tf.keras.Input(shape=(1,), name='Umur (bulan)')
jenis_kelamin_input = tf.keras.Input(shape=(1,), name='Jenis Kelamin')
tinggi_badan_input = tf.keras.Input(shape=(1,), name='Tinggi Badan (cm)')
berat_badan_input = tf.keras.Input(shape=(1,), name='Berat Badan (kg)')

# Normalize inputs
normalized_umur = tf.keras.layers.Lambda(lambda x: (x - umur_mean) / umur_std, name='normalize_umur')(umur_input)
normalized_tinggi_badan = tf.keras.layers.Lambda(lambda x: (x - tinggi_badan_mean) / tinggi_badan_std, name='normalize_tinggi_badan')(tinggi_badan_input)
normalized_berat_badan = tf.keras.layers.Lambda(lambda x: (x - berat_badan_mean) / berat_badan_std, name='normalize_berat_badan')(berat_badan_input)

# Combine all inputs
combined_inputs = tf.keras.layers.Concatenate(name='combine_input')([normalized_umur, jenis_kelamin_input, normalized_tinggi_badan, normalized_berat_badan])

# Add hidden layers
x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer='l2')(combined_inputs)
x = tf.keras.layers.Dropout(0.35)(x)
x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer='l2')(x)
x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer='l2')(x)
x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer='l2')(x)
x = tf.keras.layers.Dropout(0.35)(x)

# Output layer
output = tf.keras.layers.Dense(4, activation='softmax', name='output')(x)

# Create the model
model = tf.keras.Model(inputs=[umur_input, jenis_kelamin_input, tinggi_badan_input, berat_badan_input], outputs=output)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(
    learning_rate=0.005,
    beta_1=0.9,
    beta_2=0.999,
    amsgrad=True,
    use_ema=True,
    ema_momentum=0.99
),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=150,
    class_weight=class_weights,
    callbacks=[early_stop]
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_ds)
print(f"Test Accuracy: {test_accuracy:.4f}")

loss, accuracy = model.evaluate(test_ds)
print("Accuracy", accuracy)

def predict_input(input_umur, input_jenis_kelamin, input_tinggi_badan, input_berat_badan):
  # Create input dictionary
  if input_jenis_kelamin == 'laki-laki':
    jenis_kelamin = 0
  else:
    jenis_kelamin = 1

  user_input = {
      'Umur (bulan)': np.array([input_umur], dtype=np.float32),
      'Jenis Kelamin': np.array([jenis_kelamin], dtype=np.float32),
      'Tinggi Badan (cm)': np.array([input_tinggi_badan], dtype=np.float32),
      'Berat Badan (kg)': np.array([input_berat_badan], dtype=np.float32)
  }

  # Make prediction with the loaded model
  predicted_class = model.predict(user_input).argmax(axis=1)
  predicted_class = predicted_class[0]

  class_labels = ['Normal', 'Stunted', 'Severely Stunted', 'Tall']
  predicted_label = class_labels[predicted_class]

  print(f'Predicted class: {predicted_label}')

jenis_kelamin = input("Masukkan jenis kelamin (laki-laki/perempuan): ")
umur = float(input("Masukkan umur (bulan): "))
tinggi_badan = float(input("Masukkan tinggi badan (cm): "))
berat_badan = float(input("Masukkan berat badan (kg): "))

predict_input(umur, jenis_kelamin, tinggi_badan, berat_badan)

"""##machine learning"""

X = df_copy2[['Umur (bulan)', 'Jenis Kelamin', 'Tinggi Badan (cm)', 'Berat Badan (kg)']]
y_stunting = df_copy2['Stunting']
y_wasting = df_copy2['Wasting']

from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score
import joblib

# Stunting
# Split the data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y_stunting, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Define models
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "XGBoost": XGBClassifier(
        objective='multi:softmax',
        num_class=len(np.unique(y_stunting)),
        learning_rate=0.1,
        max_depth=6,
        n_estimators=100,
        random_state=42
    )
}

# Train and evaluate each model
best_accuracy = 0
best_model = None
best_model_name = ""

print("--- Training and Evaluating Models for Stunting ---")
for name, model in models.items():
    model.fit(X_train, y_train)

    # Validate the model
    y_val_pred = model.predict(X_val)
    val_accuracy = accuracy_score(y_val, y_val_pred)
    print(f"Validation Accuracy for {name}: {val_accuracy:.4f}")
    print(f"Validation Classification Report for {name}:\n", classification_report(y_val, y_val_pred))

    # Check if this is the best model so far
    if val_accuracy > best_accuracy:
        best_accuracy = val_accuracy
        best_model = model
        best_model_name = name

# Print and save the best model name
print(f"\nBest Model for Stunting: {best_model_name}")

"""krn si xgb sama rf itu overfit jd langsung ambil log regression tapi di tuning hyperparameter biar akurasinya agak naik"""

# Pipeline stunting
pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('over', SMOTE(random_state=42)),
    ('under', RandomUnderSampler(random_state=42)),
    ('clf', LogisticRegression(random_state=42, multi_class='multinomial'))
])

# tuning parameter
param_distributions = {
    'clf__C': uniform(0.01, 10),
    'clf__solver': ['lbfgs', 'saga'],
    'clf__max_iter': [500, 1000],
    'clf__penalty': ['l2']
}

random_search = RandomizedSearchCV(
    pipeline,
    param_distributions=param_distributions,
    n_iter=5,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

random_search.fit(X_train, y_train)

best_model = random_search.best_estimator_
best_model_name = "Logistic Regression (pipeline SMOTE + RUS)"

y_test_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"\nSelected Model: {best_model_name}")
print(f"Best Params: {random_search.best_params_}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Classification Report:\n{classification_report(y_test, y_test_pred)}")

# Save full pipeline model
joblib.dump(best_model, 'best_model_stunting.joblib')

joblib.dump((X_test, y_test), 'testset_stunting.joblib')

#Wasting
X_train, X_temp, y_train, y_temp = train_test_split(X, y_wasting, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Train and evaluate each model
best_accuracy = 0
best_model = None
best_model_name = ""

print("\n--- Training and Evaluating Models for Wasting ---")
for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train, y_train)

    # Validate the model
    y_val_pred = model.predict(X_val)
    val_accuracy = accuracy_score(y_val, y_val_pred)
    print(f"Validation Accuracy for {name}: {val_accuracy:.4f}")
    print(f"Validation Classification Report for {name}:\n", classification_report(y_val, y_val_pred))

    # Check if this is the best model so far
    if val_accuracy > best_accuracy:
        best_accuracy = val_accuracy
        best_model = model
        best_model_name = name

# Evaluate the best model on the test set
print(f"\nBest Model for Wasting: {best_model_name}")
y_test_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy for {best_model_name}: {test_accuracy:.4f}")
print(f"Test Classification Report for {best_model_name}:\n", classification_report(y_test, y_test_pred))

# Save the best model and scaler
joblib.dump(best_model, 'best_model_wasting.joblib')
joblib.dump(scaler, 'scaler_wasting.joblib')

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from scipy.stats import uniform
import joblib

X_train, X_temp, y_train, y_temp = train_test_split(X, y_wasting, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('over', SMOTE(random_state=42)),
    ('clf', LogisticRegression(random_state=42, multi_class='multinomial'))
])

#tuning parameter
param_distributions = {
    'clf__C': uniform(0.01, 1),
    'clf__solver': ['lbfgs', 'saga'],
    'clf__max_iter': [500, 1000],
    'clf__penalty': ['l2']
}

random_search = RandomizedSearchCV(
    pipeline,
    param_distributions=param_distributions,
    n_iter=5,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

random_search.fit(X_train, y_train)

best_model = random_search.best_estimator_
best_model_name = "Logistic Regression (wasting pipeline SMOTE + RUS)"

y_test_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"\nSelected Model: {best_model_name}")
print(f"Best Params: {random_search.best_params_}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Classification Report:\n{classification_report(y_test, y_test_pred)}")

# save full pipeline
joblib.dump(best_model, 'best_model_wasting.joblib')
joblib.dump((X_test, y_test), 'testset_wasting.joblib')

def evaluate_model(name, model, X_test, y_test):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"\n--- Evaluation Report for {name} ---")
    print(f"Accuracy: {acc:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    return acc

best_model_stunting = joblib.load('best_model_stunting.joblib')
X_test_stunting, y_test_stunting = joblib.load('testset_stunting.joblib')

best_model_wasting = joblib.load('best_model_wasting.joblib')
X_test_wasting, y_test_wasting = joblib.load('testset_wasting.joblib')

acc_stunting = evaluate_model("Stunting", best_model_stunting, X_test_stunting, y_test_stunting)
acc_wasting = evaluate_model("Wasting", best_model_wasting, X_test_wasting, y_test_wasting)

def load_model_pipeline(model_name):
    model_file = f'best_model_{model_name}.joblib'
    model = joblib.load(model_file)
    return model

def get_user_input():
    try:
        # Input jenis kelamin
        jenis_kelamin = input("Masukkan jenis kelamin (laki-laki/perempuan): ").lower()
        if jenis_kelamin == 'laki-laki' or 'laki - laki':
            jenis_kelamin_encd = 0
        elif jenis_kelamin == 'perempuan':
            jenis_kelamin_encd = 1
        else:
            raise ValueError("Jenis kelamin tidak valid. Harap masukkan 'laki-laki' atau 'perempuan'.")

        # Input umur
        umur = float(input("Masukkan umur (bulan): "))
        if umur <= 0:
            raise ValueError("Umur harus lebih besar dari 0.")

        # Input tinggi badan
        tinggi_badan = float(input("Masukkan tinggi badan (cm): "))
        if tinggi_badan <= 0:
            raise ValueError("Tinggi badan harus lebih besar dari 0.")

        # Input berat badan
        berat_badan = float(input("Masukkan berat badan (kg): "))
        if berat_badan <= 0:
            raise ValueError("Berat badan harus lebih besar dari 0.")

        # Return input sebagai array NumPy
        return np.array([[umur, jenis_kelamin_encd, tinggi_badan, berat_badan]])

    except ValueError as e:
        print(f"Error: {e}")
        return None

def get_user_input():
    try:
        # Input jenis kelamin
        jenis_kelamin = input("Masukkan jenis kelamin (laki-laki/perempuan): ").strip().lower()
        if jenis_kelamin in ['laki-laki', 'laki laki', 'laki']:
            jenis_kelamin_encd = 0
        elif jenis_kelamin in ['perempuan', 'wanita']:
            jenis_kelamin_encd = 1
        else:
            raise ValueError("Jenis kelamin tidak valid. Harap masukkan 'laki-laki' atau 'perempuan'.")

        # Input umur
        umur = float(input("Masukkan umur (dalam bulan): "))
        if umur <= 0:
            raise ValueError("Umur harus lebih besar dari 0.")

        # Input tinggi badan
        tinggi_badan = float(input("Masukkan tinggi badan (dalam cm): "))
        if tinggi_badan <= 0:
            raise ValueError("Tinggi badan harus lebih besar dari 0.")

        # Input berat badan
        berat_badan = float(input("Masukkan berat badan (dalam kg): "))
        if berat_badan <= 0:
            raise ValueError("Berat badan harus lebih besar dari 0.")

        # Return sebagai array NumPy
        return np.array([[umur, jenis_kelamin_encd, tinggi_badan, berat_badan]])

    except ValueError as e:
        print(f"Input error: {e}")
        return None

def predict_stunting():
    model = load_model_pipeline("stunting")
    user_input = get_user_input()
    if user_input is None:
        return

    predicted_class = model.predict(user_input)[0]
    class_labels = ['Normal', 'Stunted', 'Severely Stunted', 'Tall']
    predicted_label = class_labels[predicted_class]

    print(f'\nHasil Prediksi untuk Stunting: {predicted_label}')
    if predicted_label in ['Stunted', 'Severely Stunted']:
        print('Anak anda terindikasi stunting. Segera konsultasikan dengan tenaga medis.')
    elif predicted_label == 'Tall':
        print('Anak memiliki tinggi di atas rata-rata. Tetap pantau pertumbuhan dan kesehatan secara berkala.')
    else:
        print('Anak dalam kondisi normal. Tetap jaga pola makan dan kesehatan.')

#nyoba

def predict_wasting():
    model = load_model_pipeline("wasting")

    user_input = get_user_input()
    if user_input is None:
        return


    predicted_class = model.predict(user_input)[0]
    class_labels = ['Normal', 'Mild Wasting', 'Moderate Wasting', 'Severe Wasting']
    predicted_label = class_labels[predicted_class]

    print(f'\nHasil Prediksi untuk Wasting: {predicted_label}')
    if predicted_label in ['Moderate Wasting', 'Severe Wasting']:
        print('Anak terindikasi wasting. Segera konsultasikan dengan tenaga medis.')
    else:
        print('Anak dalam kondisi normal. Tetap jaga pola makan dan kesehatan.')

if __name__ == "__main__":
    for i in range(7):
        print(f"\n--- Iterasi {i+1} ---")
        choice = input("Apa yang ingin Anda cek? (stunting/wasting): ").lower()
111
        if choice == 'stunting':
            predict_stunting()
        elif choice == 'wasting':
            predict_wasting()
        else:
            print("Pilihan tidak valid. Silakan masukkan 'stunting' atau 'wasting'.")

"""modelnya udh cukup oke